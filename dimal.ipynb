{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from librep.estimators.dimal import DIMAL\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from librep.datasets.har.loaders import PandasMultiModalLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_view_path = Path(\"/home/lopani/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-m4-experiments/preliminary_analysis/datasets_preprocessing/data/processed/KuHar/balanced_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw, validation_raw, test_raw = PandasMultiModalLoader(processed_view_path).load(\n",
    "    label=\"standard activity code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(train_raw[:][0]).double()\n",
    "y = torch.from_numpy(train_raw[:][1].values).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0152e-02, -4.7909e-03, -1.6946e-03,  ..., -1.4326e-02,\n",
       "         -9.1198e-03, -5.4532e-03],\n",
       "        [-1.0230e-02,  1.4968e-02, -5.2134e-02,  ..., -3.1574e-03,\n",
       "         -6.7523e-04, -3.6091e-04],\n",
       "        [ 4.4708e-02, -4.7935e-02,  5.6565e-02,  ..., -6.9652e-03,\n",
       "         -1.6596e-03,  2.1321e-04],\n",
       "        ...,\n",
       "        [-2.2779e+00, -1.1530e+00, -1.3839e+00,  ..., -8.4796e-01,\n",
       "         -5.5966e-01, -3.6623e-01],\n",
       "        [ 1.1538e+00,  4.3343e+00, -6.3352e+00,  ..., -1.1036e-01,\n",
       "         -5.2955e-02,  2.2454e-01],\n",
       "        [-2.4412e+00, -5.2468e+00, -1.2993e+01,  ...,  1.4905e-02,\n",
       "         -2.5568e-01, -3.8717e-01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lopani/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | input_layer   | Linear     | 92.4 K\n",
      "1 | hidden_layers | ModuleList | 41.2 K\n",
      "2 | output_layer  | Linear     | 130   \n",
      "---------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.535     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ee31f87f0e4f34ab780b3f172073aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, (x\u001b[39m.\u001b[39;49mdouble(), y\u001b[39m.\u001b[39;49mdouble()))\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1103\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1103\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1105\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1182\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1182\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1205\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1204\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(dataloader, batch_to_device\u001b[39m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:213\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    212\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 213\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(kwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    217\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m     85\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\n\u001b[1;32m     86\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbatch_idx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(optimizers, kwargs)\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:202\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, optimizers: List[Tuple[\u001b[39mint\u001b[39m, Optimizer]], kwargs: OrderedDict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_kwargs(kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hiddens)\n\u001b[0;32m--> 202\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position])\n\u001b[1;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    205\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[1;32m    206\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:249\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[1;32m    241\u001b[0m         closure()\n\u001b[1;32m    243\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[39m# the `batch_idx` is optional with inter-batch parallelism\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[1;32m    251\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:370\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    362\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m PyTorch Lightning has been deprecated in v1.9.0 and will be removed in v2.0.0.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m return True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39musing_native_amp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprecision_plugin, MixedPrecisionPlugin)\n\u001b[0;32m--> 370\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    371\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    372\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    373\u001b[0m     batch_idx,\n\u001b[1;32m    374\u001b[0m     optimizer,\n\u001b[1;32m    375\u001b[0m     opt_idx,\n\u001b[1;32m    376\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    377\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator, TPUAccelerator),\n\u001b[1;32m    378\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    379\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[1;32m    380\u001b[0m )\n\u001b[1;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    383\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1347\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m   1346\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1347\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1349\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1744\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_lbfgs)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   1666\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1667\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1674\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1675\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m \u001b[39m    each optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \n\u001b[1;32m   1743\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1744\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:169\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:234\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(\n\u001b[1;32m    235\u001b[0m     optimizer, model\u001b[39m=\u001b[39;49mmodel, optimizer_idx\u001b[39m=\u001b[39;49mopt_idx, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    236\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:119\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> 119\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/torch/optim/sgd.py:130\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 130\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    132\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    133\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:105\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m     93\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     94\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     98\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     99\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:149\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    150\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:135\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 135\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:419\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \n\u001b[1;32m    412\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m    A ``ClosureResult`` containing the training step output.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m training_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    420\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()\n\u001b[1;32m    422\u001b[0m model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mtraining_step_end\u001b[39m\u001b[39m\"\u001b[39m, training_step_output)\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1485\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1485\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1487\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:378\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[1;32m    377\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TrainingStep)\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/src/librep/estimators/dimal.py:27\u001b[0m, in \u001b[0;36mDIMAL.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     26\u001b[0m     inputs, targets \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> 27\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(inputs)\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/src/librep/estimators/dimal.py:16\u001b[0m, in \u001b[0;36mDIMAL.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layer(x)\n\u001b[1;32m     17\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_layers:\n\u001b[1;32m     18\u001b[0m         x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(layer(x))\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Doutorado/UNICAMP/H.IAAC-Meta4/hiaac-librep/.librep-venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "# Initialize the DIMAL model\n",
    "input_dim = 360\n",
    "hidden_dims = [256, 128, 64]\n",
    "output_dim = 2\n",
    "model = DIMAL(input_dim, hidden_dims, output_dim)\n",
    "\n",
    "# Train the model\n",
    "trainer = pl.Trainer(max_epochs=50)\n",
    "trainer.fit(model, (x.double(), y.double()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "vals = model(X).detach().numpy()\n",
    "x, y = vals[:, 0], vals[:, 1]\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".librep-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e57cbd9857f73b6186314efc0497b85fc81e429910d4dfbf03f56c852bfb6a26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
