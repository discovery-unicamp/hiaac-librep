{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb03c8ae-1724-46e4-9abf-ea79705024f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/usr/local/lib/python3.8/dist-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/usr/local/lib/python3.8/dist-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/usr/local/lib/python3.8/dist-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "2023-09-12 20:19:51.463574: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-12 20:19:51.593458: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-12 20:19:52.116007: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-09-12 20:19:52.116073: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-09-12 20:19:52.116079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  \n",
    "from librep.base.estimator import Estimator\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils.standartized_balanced import StandardizedBalancedDataset\n",
    "import os,torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_data(dataset_name,sensors,normalize_data):    \n",
    "    working_directory=f\"result/{dataset_name}/\"\n",
    "    data_folder = f\"../data/data/standartized_balanced/{dataset_name}/\"\n",
    "    dataset = StandardizedBalancedDataset(data_folder, sensors=sensors)\n",
    "    X_train, y_train,X_test, y_test,X_val, y_val = dataset.get_all_data(normalize_data=normalize_data, resize_data=True)\n",
    "    return X_train, y_train,X_test, y_test,X_val, y_val\n",
    "        #print(f\"shape: X_train {X_train.shape} --- X_test {X_test.shape} --- X_test {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6431cd4-10a4-4541-8838-49c3c4bb78f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from models.linear_model import LinearModel\n",
    "from models.simclr_head import SimCLRHead\n",
    "from simclr import SimCLR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Estimator(Estimator):\n",
    "    def __init__(self,trained_simclr_model,input_shape,\n",
    "                batch_size_head,transform_funcs,temperature_head,epochs_head,\n",
    "                 save_model=False,verbose=0,total_epochs=50,\n",
    "                 batch_size=32,lr=0.001,classificator='full'):\n",
    "       \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.loaded_simclr_model=trained_simclr_model\n",
    "        self.batch_size=batch_size\n",
    "        self.lr=lr\n",
    "        self.total_epochs=total_epochs\n",
    "        self.verbose=verbose\n",
    "        self.classificador=classificator\n",
    "        if trained_simclr_model==None:\n",
    "            simclr_head = SimCLRHead(input_shape).to(device)\n",
    "            self.simclr = SimCLR(model=simclr_head,batch_size=batch_size_head,\n",
    "                                        transform_funcs=transform_funcs,\n",
    "                                        temperature=temperature_head, epochs=epochs_head,\n",
    "                                        is_transform_function_vectorized=True,\n",
    "                                        verbose=verbose,device=device)\n",
    "        else:\n",
    "            self.simclr=trained_simclr_model\n",
    "            \n",
    "\n",
    "        \n",
    "    def fit(self,X_train,y_train,X_val,y_val):\n",
    " \n",
    "        \n",
    "        trained_simclr_model,epoch_wise_loss = self.simclr.fit(X_train)\n",
    "\n",
    "\n",
    "        # Define the number of classes for your downstream task\n",
    "        num_classes = len(y_train[1])\n",
    "        input_shape = X_train[1].shape  # Define your input shape        \n",
    "        \n",
    "        \n",
    "        train_labels_int = torch.argmax(torch.tensor(y_train), dim=1)\n",
    "        val_labels_int = torch.argmax(torch.tensor(y_val), dim=1)\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), train_labels_int)\n",
    "        val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), val_labels_int)\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        if self.classificador=='full':            \n",
    "            evaluation_model = LinearModel(trained_simclr_model, num_classes, ).to(self.device)\n",
    "        self.best_model=evaluation_model\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Define loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(evaluation_model.parameters(), self.lr)\n",
    "\n",
    "        best_accuracy = 0.0\n",
    "        best_model_path = \"best_linear_evaluation_model_est.pth\"\n",
    "\n",
    "        for epoch in range(self.total_epochs):\n",
    "            evaluation_model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = evaluation_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            if(self.verbose):\n",
    "                print(f\"Epoch [{epoch+1}/{self.total_epochs}] - Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "            # Evaluate the model on the val set\n",
    "            evaluation_model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = evaluation_model(inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_accuracy = correct / total\n",
    "            if(self.verbose):\n",
    "                print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            # Save the model if it's the best model so far\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                torch.save(evaluation_model.state_dict(), best_model_path)\n",
    "                print(f\"Best model saved with val accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "        print(\"Training finished.\")\n",
    "        # Load the saved model state dict\n",
    "        self.best_model.load_state_dict(torch.load(best_model_path))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "       \n",
    "    def predict(self, input_data):\n",
    "        \"\"\"\n",
    "        Predict class labels for input data.\n",
    "\n",
    "        Args:\n",
    "            input_data (numpy.ndarray or torch.Tensor): Input data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted class labels.\n",
    "        \"\"\"\n",
    "        # Ensure input_data is a torch.Tensor\n",
    "        if not isinstance(input_data, torch.Tensor):\n",
    "            input_data = torch.tensor(input_data, dtype=torch.float32)\n",
    "\n",
    "        # Move input data to the device (GPU or CPU)\n",
    "        input_data = input_data.to(self.device)\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        self.best_model.eval()\n",
    "\n",
    "        # Perform predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.best_model(input_data)\n",
    "            predicted_classes = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Convert the PyTorch tensor to a NumPy array\n",
    "        predicted_classes_np = predicted_classes.cpu().numpy()\n",
    "\n",
    "        return predicted_classes_np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f455d-3102-4df9-beb8-2d885ec8c146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
